{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the Numpy repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural Net should be considered as a bridge or connection, which connects the inputs with the outputs.\n",
    "\n",
    "but theoritically its a composite function that acts upon the input to give an output.\n",
    "we will learn about this in the coming code snippets.\n",
    "\n",
    "We will divide this session into two parts\n",
    "\n",
    "1. Neural net (This will contain all necessary components required in a neural net as below)\n",
    "   a. Nodal weights\n",
    "   b. Activation function\n",
    "   c. Training function\n",
    "   d. Prediction function\n",
    "2. Main function (This function will call the Neural net with an input and gets back a predicted output)\n",
    "\n",
    "As we move along the code we will learn about each and every individual components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the basic of a class in python is its init function, which initializes the class variables associated with the entire class.\n",
    "For further clarity on the init function, please refer the below link.\n",
    "\n",
    "https://micropyramid.com/blog/understand-self-and-__init__-method-in-python-class/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1) # this will make sure random data will be same when ever the python script is run.\n",
    "       # this will just assign the nodal weights with some random numbers\n",
    "       # Multiplying by 2 and subtracting by 1 is done, only to boost this random values to avoid very small numbers.\n",
    "        self.nodal_weights = 2 * np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodal weights are generally a function or a matrix which acts upon the input to provide efficient output.\n",
    "We will fine tune this nodal weights using gradient descend method by trying to reduce the error value calculated between the predicted and the actual output.\n",
    "\n",
    "To know more about the concept of gradient descend, please check the below link.\n",
    "\n",
    "https://machinelearningmastery.com/gradient-descent-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sigmoid(self,X):\n",
    "        return (1/(1 + np.exp(-X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function is called the activation function, as the name suggest it activates the neuron that we are trying to produce.\n",
    "\n",
    "This is called sigmoid activation function, which outputs all the input values between 0 and 1.\n",
    "\n",
    "There by transforming the inputs into a more generalised form, which helps us to understand relationship between the input and the output.\n",
    "\n",
    "This sigmoid kind of activation is helpful in the binary classification kind of problems where the output is either 0 or 1.\n",
    "In this method too we are trying to perform a binary classification.\n",
    "\n",
    "To know more about the sigmoid activation functions and all other types of activation functions, please check the below link\n",
    "https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def der_sigmoid(self,X):\n",
    "        return (X*(1-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function is nothing but the derivative or differentiation of the activation function.\n",
    "If interested, you can perform differentiation on the activation function and check whether the above derivative is correct.\n",
    "\n",
    "To know more about  \"why a derivative is required?\", please check the above link on the gradient descent to understand better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def brain(self,inputs):\n",
    "\n",
    "        return self.sigmoid(np.dot(inputs, self.nodal_weights))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above brain function is where the important and interesting stuff happens.\n",
    "\n",
    "This is where input and nodal weights get combined and gets activated through sigmoid. \n",
    "So the composite function will be as below\n",
    "\n",
    "output = 1/(1+ exp(-(inputs.nodel_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training(self, actual_inputs, actual_outputs, no_of_iterations):\n",
    "\n",
    "        for iterations in range(no_of_iterations):\n",
    "\n",
    "            output =self.brain(actual_inputs)#predicts the output\n",
    "\n",
    "            error = actual_outputs - output#Compares with the actual output and calculates a error value\n",
    "\n",
    "            tuning = np.dot(actual_inputs.T,error * self.der_sigmoid(output))#Based on the error value, \n",
    "\n",
    "            self.nodal_weights += tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training function is nothing but how a neural network learns\n",
    "\n",
    "Skipping the indepth analysis on how the neural network learns , as it will be covered in the gradient descend link, shared above. \n",
    "\n",
    "The basic intuition behind this training function is that, by calling brain function in iteration, neural net does the following as below\n",
    "\n",
    "1. Predicts the output\n",
    "2. Compares the predicted output with the actual output and calculates a error value\n",
    "3. Based on the error value, it updates the nodal_weights using gradient descend.\n",
    "4. The above 3 process continues untill the iteration is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    obj_neural_net = neural_net()\n",
    "\n",
    "    print (\"random nodal weights\")\n",
    "    print (obj_neural_net.nodal_weights)\n",
    "\n",
    "    actual_inputs = np.array([[0,0,1],[1,1,1],[1,0,1],[0,1,1]])\n",
    "    actual_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "    obj_neural_net.training(actual_inputs,actual_outputs,10000)\n",
    "\n",
    "    print (\"new nodal weights\")\n",
    "    print (obj_neural_net.nodal_weights)\n",
    "\n",
    "    print (\"new data set\")\n",
    "    print (obj_neural_net.think(np.array([1,1,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code, we will get the below output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random nodal weights\n",
    "[[-0.16595599]\n",
    " [ 0.44064899]\n",
    " [-0.99977125]]\n",
    "new nodal weights\n",
    "[[ 9.67299303]\n",
    " [-0.2078435 ]\n",
    " [-4.62963669]]\n",
    "new data set\n",
    "[0.9999225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the displayed outputs, if we observe the new nodal weights output , the first value is a large number and has positive value.\n",
    "whereas the other 2 values are negative.\n",
    "\n",
    "and if we look closely on the input passed,\n",
    "\n",
    "[0,0,1],\n",
    "[1,1,1],\n",
    "[1,0,1],\n",
    "[0,1,1]\n",
    "\n",
    "the first column of the input is same as that of the output.\n",
    "\n",
    "\n",
    "So through training, our neural net has learned that only the first column of the input is only contributing to the output.\n",
    "hence it got a positive value while other 2 columns got a negative value.\n",
    "\n",
    "And when this passed in the sigmoid function sigmoid(np.dot(inputs, self.nodal_weights),\n",
    "any value that is above 1 is considered as 1 and any value below zero is considered as zero.\n",
    "\n",
    "So the first column is multiplied by value approximately equal to 1  and other columns are multiplied by value approximately equal to 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "And Voila!!! we have our simple working neural network.\n",
    "So we can conclude by saying that our simple neural net have learned the relation between our input and output successfully.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
